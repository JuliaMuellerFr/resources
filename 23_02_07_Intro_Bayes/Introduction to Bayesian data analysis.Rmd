---
title: "Introduction to Bayesian data analysis"
author: "Julia Müller"
date: "8 February 2023"
output: html_document
---


**Preparation**
Make sure the necessary packages are installed. {brms} needs a C++ compiler to run Bayesian models. 
- On Windows, RTools contains this and can be installed here: https://cran.r-project.org/bin/windows/Rtools/
Make sure to add it to the PATH variable (should be an option during installation).
- On Mac, install XCode via the app store.

You can test whether your {brms} installation was successful by trying to run the following code block - if it fits a model and gives you a model summary, everything worked.
```{r}
library(brms)
BRM1 <- brm(weight ~ Diet, data = ChickWeight)
summary(BRM1)
```


**Structure**
- Theory: Introducing Bayesian approach
- Demonstration 1: Reading times
- Demonstration 2: Direction of gestures
- Comparison: Frequentist and Bayesian approaches


# Theory

## Main steps of Bayesian data analysis
(1) Prior knowledge about the parameters (i.e., likely values for intercept, slopes, etc. of the model)
(2) Likelihood function - your data
(3) Posterior distribution: Updated knowledge - compromise between the prior and the data


## Quick history lesson
- Bayesian analysis as a concept is old: basics first described in 1763
- Bayes' theorem published in 1825
- computationally expensive -> became more popular recently, with increase in computing power and appearance of suitable programming languages


## Maths lesson: Probability distributions
- distributions tell us which values are possible for a certain parameter/variable
- and how are these values distributed: which values are more or less likely
-> probability of occurrence of different outcomes: how likely are data points in different ranges?

### Examples
- Which outcomes are possible when throwing a coin? How likely are they each?
- Which outcomes are possible when throwing a six-sided die? How likely are they each?
- Which values are possible when counting the number of pets in each household? Which values are most likely?
- Which values are possible when recording reading times?


### Some common distributions
Continuous:
- *Uniform distribution*: equal probability for any value in range
  - examples: dice rolls, coin throws
  - can be for continuous or discrete data
- *"Normal" (Gaussian) distribution*
  - looks like a "bell curve"
  - continuous data
  - centered on the mean, symmetric on both sides of the mean
  - most data lies closely to the mean
  - parameters: mean and standard deviation
  - typical data: reaction times, reading times (SPR), scores on e.g. test (if measured in a continuous way)
  
Discrete:
- *Bernoulli distribution*
  - two options (true/false, success/failure)
  - typical data: choice between variants (e.g. constructions, words, pronunciation options...)
  - but in contrast to data from a uniform distribution (coin throws), the two options don't have the same probability
- *Poisson distribution*
  - number of discrete events
  - typical data: counts (e.g. word counts, counts of discourse markers)
  - whole numbers
  - bounded by 0 -> negative values not possible
  - only one parameter called lambda

Nice visualisations of distributions -- click on "Discrete and Continuous":
https://seeing-theory.brown.edu/probability-distributions/index.html


### Distributions in frequentist statistics
- generalised linear models: What's the distribution for data-generating process - which distribution is the dependent variable drawn from?
    - normal (Gaussian) distribution -> linear regression
    - Bernoulli distribution -> logistic regression
    - Poisson distribution -> Poisson regression
- this is also relevant for Bayesian models, but we additionally need to think about distributions for the priors


## Bayes' rule

Bayes' rule for two discrete events A and B:
![Bayes' rule two discrete events](img/BayesAB.png)  
P(A) = probability of event A
P(B) = probability of event B
P(A|B) = probability of event A given event B
P(B|A) = probability of event B given event A

Example:
You'd like to set up  a picnic today, but in the morning, it's cloudy.
- 50% of rainy days start off cloudy
- 40% of days start off cloudy
- this month, only 3 of 30 days (10%) tend to be rainy

What is the chance of rain today?
or: P(Rain|Cloud) = chance of rain given that it is cloudy

P(Rain|Cloud) = (P(Cloud|Rain) * P(Rain)) / P(Cloud)

P(Rain) is probability of rain = 10%
P(Cloud|Rain) is probability of a cloudy start to the day, given that it later rains = 50%
P(Cloud) is probability of cloud = 40%
P(Rain|Cloud) =  (0.1 * 0.5)/0.4 = 0.125
-> 12.5% chance of rain


Bayes' rule for probability distributions:
![Bayes' rule for probability distributions](img/BayesPD.png)


theta = parameter we're trying to estimate

Put in words:
*Posterior p() = (Likelihood x Prior)/Marginal Likelihood*

- Posterior: probability distribution of the parameters conditional on the data and prior
- Likelihood = functions that assign probabilities or relative frequencies to all events in a sample space, written as f(x)
- Prior: initial probability distribution (*before* seeing the data)
- Marginal likelihood (= the constant, the denominator in the formula): standardises the posterior distribution to ensure that the area under the curve of the distribution sums up to 1 -> is a valid probability distribution

Without the constant:
Posterior proportional to Likelihood x Prior (i.e. the product of Likelihood and Prior)


### Examples for different priors

The posterior is a compromise between the prior and the data/likelihood
![](img/prior0.png)
- e.g., you set up a difficult exam and your expectation (= prior) is that students get only 1 or 2 questions right, maybe because that was the case for last term's students
- but this term, most people answer 5 to 7 questions correctly (= likelihood/data)
- so you shift your expectations accordingly (= posterior): next term, students might get 3 to 4 questions right

![](img/prior1.png)

![](img/prior2.png)


## Types of priors

- priors need to be set for every parameter in the model
-> intercept, slope(s), sigma (error term)
-> random effects

### Flat, uninformative priors
- choosing as uninformative priors as possible to 'let the data speak for itself' (uniform distribution)
- but:
  - are unrealistic (equal weight to any value)
  - make sampling slower -> might lead to convergence problems

### Regularizing priors
- also called weakly informative/mildly informative priors
- priors that downweight extreme values
- not very informative, mostly letting the likelihood dominate in determining the posteriors
- incorporate mild skepticism
- are theory-neutral: they usually do not bias the parameters to values supported by any prior belief or theory
- help stabilize computation
- especially important for small sample sizes to prevent overfitting (= model doesn't generalise well to new data)

### Principled and informative priors
- encode all (or most of) the theory-neutral information that we do have
- e.g., if we know that an effect has usually been found to be positive (or negative), we can express that in the prior


## Sensitivity analysis

= settling on a prior that makes sense for the data, but the model should be re-run with different priors to see if any of them change the model output
- with lots of data, likelihood will dominate in determining the posterior distributions
- more complex models require more data
- sensitivity analysis useful to determine the extent to which the posterior is influenced by our priors
  - try different priors and either
    - verify that the posterior doesn’t change drastically
    - report how the posterior is affected by some specific priors


**Takeaways**
(1) The posterior is a compromise between the prior and the likelihood
(2) Priors are more influential with smaller data sets (relative to how complex the model structure is)


## Deriving the posterior through sampling

- For simple problems, we can calculate the posterior
- But this is not realistic for most real data analyses
- Different sampling algorithms are available 
- A commonly used one is Markow-Chain-Monte-Carlo (MCMC)

Sampling and convergence in a nutshell
- Aim: to approach the posterior distribution
- Uses several chains which are independent of each other
- Chains start in random locations
- In each iteration, they each take one sample
- Samples at the beginning do not belong to the posterior distribution and are discarded (warm-up)
- Eventually, the chains end up in the vicinity of the posterior distribution
- From that point on the samples will belong to the posterior
- Given enough samples we will have a good approximation of the real posterior distribution 

Commonly used analogy:
- Several blindfolded wanderers (= chains) start in different locations in a desert and each try to find the highest peak (= posterior distribution)
- They take steps (= iterations) to find the highest spot
- At first, they are lost and wandering aimlessly (= warm-up)
- But after a while, they start to approach the peak (so these are the steps/iterations we keep)
- A map (= prior information) can help them know where to start looking for the peak
- All wanderers should get close to the peak - if they end up in different spots (= the chains aren't mixing) we can't be sure we found the highest peak (= the estimate of the posterior is likely unreliable)


## Model formula
- similar to fitting frequentist models 
  - notation: dep_var ~ 1 + ind_var
  - interactions with * and/or :
  - random effects
  - family argument (which distribution is the dependent variable drawn from)
- additional arguments
  - prior (if not specified, brms will pick priors)
  - chains (refers to the number of independent runs for sampling - default 4, should be at least 3)
  - iter (number of iterations that the sampler makes to sample from the posterior distribution of each parameter - default 2000)
  - warmup (number of iterations from the start of sampling that are eventually discarded - default half of iter)


# Demonstration

## Software and packages
```{r}
# Packages

library(tidyverse) # make sure to load these in the correct order!
library(brms) #allows us to use the Stan programming language without having to learn it
library(tidybayes)


# Settings

# to speed up modeling time, tell R to use several of your PC's cores
parallel::detectCores() # how many cores does your PC have?
options(mc.cores = 3) # how many should be used?

# set a seed for reproducibility
set.seed(2023)
```


## Example 1: Reading data

### Background and data
```{r}
reading_df <- read_csv("data/reading_data.csv") %>% 
  mutate(across(-rt, as.factor))

sample_n(reading_df, 20)
```
- contains reading times on words
- participants are L2 learners
- condition: some have received training on L2
- research question: Are participants who received training faster than those who did not receive training?

```{r}
reading_df %>% 
  count(condition, participant) # condition is between-subject

reading_df %>% 
  group_by(condition) %>% 
  summarise(mean(rt))
```


### Setting up the model
- model with brms default priors first (not recommended)
```{r}
reading_mdl_def <- brm(rt ~ condition + 
                         (1 + condition|item) +    
                         (1|participant),
  family = gaussian(),
  data = reading_df,
  iter = 4000,
  warmup = 2000,
  chains = 3
)

#save(reading_mdl_def, file = "models/reading_mdl_def.RData")
load("models/reading_mdl_def.RData")
```


#### Convergence diagnostics
- check the model to see if it converged
- Rhat: ratio approximately 1 when within- and between-variability of the chains is roughly the same -> they converge
- Bulk_ESS: should be larger than 10% of total number of samples

```{r}
reading_mdl_def
```

Visual inspection:
- for each parameter, we see
  - left: a density plot of the estimate
  - right: the chains and whether they mixed (caterpillar plots)
    -> did they find the same/a very similar solution?
    if not, model not reliable
```{r}
plot(reading_mdl_def)
```


Which priors did brms pick?
```{r}
prior_summary(reading_mdl_def)
```
- lists all parameters in the model which need priors and their priors
  - intercept and slopes (class = b)
  - sigma (error)
  - mixed effects models: intercept adjustments with sd, slope adjustments with sd, correlation parameter
- tells us the distribution with parameters for each prior
  - Student t-distribution for the intercept and random effects
    - looks similar to the normal distribution, but heavier tails
    - centered on 0
  - flat priors for the slopes -> any value equally likely (but this is not realistic!)
-> brms picks sensible regularising priors, taking into account link functions and the scale of the data
-> but we should still set our own priors, especially for the slopes


### Bodo Winter's approach to prior selection
- use default priors on all parameters except coefficients
- use weakly informative priors on coefficients to incorporate "mild skepticism"/make model more conservative than models with uniform priors/frequentist models
- typically use a normal distribution
-> centre prior on 0 and choose SD that makes sense for the scale of the data and the link function
- for SD: think about where 68%/95% of the data fall
-> 68% of data fall one standard deviation to either side of mean
  95% of data within two standard deviations
e.g., mean = 0, SD = 0.5 -> 68% within -0.5 and 0.5
  95% of data within -1 and 1

```{r}
prior_p <- ggplot(data = tibble(x = c(-5, 5)), aes(x = x)) +
  stat_function(fun = dnorm, n = 101,
                args = list(mean = 0, sd = 1),
                aes(color = 'red'), size = 1.15) +
  stat_function(fun = dnorm, n = 101,
                args = list(mean = 0, sd = 0.5), linetype = 2,
                aes(color = 'blue'), size = 0.65) +
  stat_function(fun = dnorm, n = 101,
                args = list(mean = 0, sd = 2), linetype = 3,
                aes(color = 'black'), size = 1.15) +
  scale_color_manual(values = c('red', 'blue', 'black'),
                     name = 'Standard deviation',
                     breaks = c('red', 'blue', 'black'),
                     labels = c('SD = 1', 'SD = 0.5', 'SD = 2')) +
  scale_x_continuous(breaks = -4:4) +
  scale_y_continuous(name = 'Probability density', expand = c(0, 0)) +
  theme_classic() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.x = element_text(size = 8),
        axis.text.y = element_text(size = 8),
        axis.title.y = element_text(face = 'bold', size = 10,
                                    margin = margin(r = 15, l = 0,
                                                    t = 0, b = 0)),
        axis.title.x = element_text(face = 'bold', size = 10,
                                    margin = margin(r = 0, l = 0,
                                                    t = 15, b = 0))) +
  coord_cartesian(ylim = c(0, 0.8)) +
  xlab('Possible slope values')
```

```{r}
prior_p
```


### Priors
Domain knowledge: What's a sensible difference in reading times between words?

```{r}
sceptical_priors <- c(prior(normal(0, 50), class = b))
```
What values are within 95% of the probability distribution of normal(0, 50)?
-> -100 and +100

Rerunning the model with those priors:
```{r}
reading_mdl_reg <- brm(rt ~ condition + 
                         (1 + condition|item) +    
                         (1|participant),
  family = gaussian(),
  data = reading_df,
  prior = sceptical_priors,
  iter = 4000,
  warmup = 2000,
  chains = 3
)

#save(reading_mdl_reg, file = "models/reading_mdl_reg.RData")
load("models/reading_mdl_reg.RData")
```

...and re-checking:
```{r}
reading_mdl_reg
plot(reading_mdl_reg)
```


### Posterior predictive checks
= sample n data sets generated from the model (which incorporated the likelihood and priors)
- generate n posterior distributions and compare them to the actual distribution
-> "descriptive adequacy" of the model
- a model should not only fit the data well
- the range of its predictions should also be reasonably constrained
- posterior predictive checking plots observed and predicted data
- can also help determine which distribution (family argument) is better fit for the data
```{r}
pp_check(reading_mdl_reg, ndraws = 100)
```

Interpretation:
- dark blue line is a density plot of the observed data
- thinner light blue lines are samples drawn from the posterior (reminder: compromise prior and data)
- should be fairly close together
- dark blue line should fall within the light blue lines
-> data and samples should be fairly similar

Here is what it should not look like:
```{r}
knitr::include_graphics("img/bad_pp_check.PNG")
```


### Model output
```{r}
contrasts(reading_df$condition)
```
-> "no training" is the reference level (model intercept)
-> slope will show the change in reading times to the training condition

```{r}
plot(reading_mdl_reg)
```
- each parameter is represented by a density plot
- it shows which values are most likely

```{r}
reading_mdl_reg
```
- the model summary shows the likeliest value for each parameter (MLE)
  - random effects = group-level effects
  - fixed effects = population-level effects
- along with the 95% credible (not confidence) intervals
  - derived from these plots
  - interpretation: we can be 95% certain that the true parameter value is between those two points, given the data and model
  - cf. confidence intervals: if we draw from the same population 100 times, then 95 times the CI will contain the true population value
  
Specifically for this model:
- 95% certain that reading times in no training condition are between 275.31 and 295.74
- slope is likely negative -> participants who had training read more quickly
- 95% certain that slope is between -19.52 and 5.78

Visualisation of model output:
```{r}
mcmc_plot(reading_mdl_reg)
mcmc_plot(reading_mdl_reg, type = "hist")
mcmc_plot(reading_mdl_reg, variable = "^b_", regex = TRUE)
```
- dot shows most likely estimate
- thicker line: 80% credible interval
- thinner line: 95% credible interval


### Model predictions
We can plot the model predictions (with 95% credible intervals):
```{r}
conditional_effects(reading_mdl_reg)
```


### Bayes factors
- method of quantifying evidence for or against hypothesis or model
- used for hypothesis testing or model comparison
- criticised by some Bayesians for binary cut-off points
![](img/BFs.png)
- can be strongly influenced by priors (even if posterior estimates/model parameters are not!)
- often requires more iterations than default


### Hypothesis testing
- we can calculate probabilities for or against hypotheses
-> one of the advantages of Bayesian models

For example: Is the slope for condition negative?
```{r}
hypothesis(reading_mdl_reg, "conditiontraining < 0")
```

Is the intercept (reading times in the no training condition) above 250?
```{r}
hypothesis(reading_mdl_reg, "Intercept > 250")
```


### Sensitivity analysis
- for a sensitivity analysis, we fit the same model with different priors to see how much the priors affect the posterior
- here: one narrower prior and one informative prior

- the regularising priors we picked are fairly wide
-> fit a model with much narrower prior on the slope, such as Normal(0, 10)
-> prior assumes that 95% of values lie between -20 and 20

- let's assume that previous research has found that the effect of training is around 10 ms: In other studies, participants read words 10 ms more quickly (on average) if they had had training
-> fit a model with an informative prior for the slope of Normal(-10, 10)
-> prior assumes that 95% of values lie between -30 and 10

Fitting models:
```{r}
reading_mdl_narrow <- brm(rt ~ condition + 
                         (1 + condition|item) +    
                         (1|participant),
  family = gaussian(),
  data = reading_df,
  prior = prior(normal(0, 10), class = "b"),
  iter = 4000,
  warmup = 2000,
  chains = 3
)

reading_mdl_info <- brm(rt ~ condition + 
                         (1 + condition|item) +    
                         (1|participant),
  family = gaussian(),
  data = reading_df,
  prior = prior(normal(-10, 10), class = "b"),
  iter = 4000,
  warmup = 2000,
  chains = 3
)

#save(reading_mdl_narrow, file = "models/reading_mdl_narrow.RData")
#save(reading_mdl_info, file = "models/reading_mdl_info.RData")
load("models/reading_mdl_narrow.RData")
load("models/reading_mdl_info.RData")
```

Comparing model summaries:
```{r}
reading_mdl_reg # -7.14 CI[-19.52, 5.78]
reading_mdl_info # -7.86 [-17.98, 1.77]
reading_mdl_narrow # -4.98 CI[-15.21, 5.34]
```


### Picking a different distribution
- reading times are often better described by a log-normal distribution
- they can't be negative
- often have a long tail (more high values)
- are often log-transformed before modeling, but we can also pick a different family argument
```{r}
ggplot(reading_df) +
  aes(x = rt) +
  geom_density() +
  theme_light()
```

The log-normal distribution might work:
```{r}
knitr::include_graphics("img/Log-normal-pdfs.png")
```

Model with log-normal instead of normal distribution and adjusted prior:
```{r}
reading_mdl_ln <- brm(rt ~ condition + 
                         (1 + condition|item) +    
                         (1|participant),
  family = lognormal(),
  data = reading_df,
  prior = prior(normal(0, 5), class = "b"),
  iter = 4000,
  warmup = 2000,
  chains = 3
)

#save(reading_mdl_ln, file = "models/reading_mdl_ln.RData")
load("models/reading_mdl_ln.RData")
```


```{r}
plot(reading_mdl_ln)
reading_mdl_ln
```

The estimates need to be exponentiated to transform them back to reading times
```{r}
exp(5.65) # intercept: no training condition
exp(5.65 - 0.02) # intercept + slope: training condition
```

The conditional effects plot does this automatically:
```{r}
conditional_effects(reading_mdl_ln)
```

We can compare the posterior predictive checks to see which model is a better choice:
```{r}
pp_check(reading_mdl_reg, ndraws = 100)
pp_check(reading_mdl_ln, ndraws = 100)
```


## Example 2: Direction of gestures

### Background and data
From a tutorial taught by Bodo Winter, with data from Winter & Duffy (2020), 'Can co-speech gesture alone carry the mental timeline?'

Participants were asked the following question:
"Next Wednesday's meeting has been moved **forward/backward** two days, what day is the meeting on now?"
-> roughly 50%/50% split between Friday and Monday

Can gestures modulate the interpretation?
- hands moving towards body -> more Monday responses?
- hands moving away from body -> more Friday responses?
2x2 design: "forward"/"backward" x gesture towards/away from body

Research questions:
- Is there an interaction between verbal and non-verbal directions?
- Which effect is stronger - language or gesture?

```{r}
E3 <- read_csv("data/direction_gestures.csv")

# Factor conversion, releveling
E3 <- E3 %>% 
  mutate(
    across(c(language, direction), as_factor),
    response = factor(response, levels = c('Monday', 'Friday')),
  )

# Sum-coding
contrasts(E3$language) <- contr.sum(2) / 2 * -1
contrasts(E3$direction) <- contr.sum(2) / 2 * -1

sample_n(E3, 10)
```


### Setting up the model

#### Default priors
```{r}
E3_mdl_default <- brm(response ~ direction * language,
                      data = E3,
                      family = bernoulli, # logistic regression
                      chains = 3,
                      warmup = 2000,
                      iter = 4000
                      )

#save(E3_mdl_default, file = 'models/E3_mdl_default.RData')
load(file = 'models/E3_mdl_default.RData')
```

Checking model convergence:
```{r}
plot(E3_mdl_default)
```
The chains haven't mixed!

```{r}
prior_summary(E3_mdl_default)
```
The default priors are flat (= uniform distribution)


#### Regularizing priors
- same approach as in example 1, but because of the link function, the estimates will be smaller
- priors should reflect that
```{r}
regularizing_priors <- c(prior(normal(0, 1), class = b))

E3_mdl_reg <- brm(response ~ direction * language,
                      data = E3,
                      family = bernoulli, # logistic regression
                      prior = regularizing_priors,
                      chains = 3,
                      warmup = 2000,
                      iter = 4000
                      )

#save(E3_mdl_reg, file = 'models/E3_mdl_reg.RData')
load(file = 'models/E3_mdl_reg.RData')
```

```{r}
plot(E3_mdl_reg)
```
The chains have mixed

```{r}
prior_summary(E3_mdl_reg)
```


### Model output

Reminder:
- "Next Wednesday's meeting has been moved **forward/backward** two days, what day is the meeting on now?"
- direction of gesture: forward or backward
- "Monday" is the reference level, so this predicts "Friday" responses

```{r}
contrasts(E3$direction)
contrasts(E3$language)

E3_mdl_reg
mcmc_plot(E3_mdl_reg)
```
Slopes:
- positive slope for direction
-> more Friday responses with forward gesture
- positive slope for language
-> more Friday responses with forward language


### Plotting predicted values
```{r}
conditional_effects(E3_mdl_reg,
                    effects = 'language:direction')
```
- "moved backward" + backward gesture: low predicted proportion of "Friday" responses, small CI -> interpreted as Monday
- "moved backward" + forward gesture: still fairly low predicted proportion of "Friday" responses, but lower than previous condition, and larger CI -> interpreted mostly as Monday
- "moved forward" + backward gesture: slightly higher predicted proportion of "Friday" responses, but still lower than 0.5, and large CI -> still mostly Monday responses
- "moved forward" + forward gesture: highest predicted proportion of "Friday" responses, but not as strong as backward + backward for Monday -> mostly interpreted as Friday


### Testing hypotheses

Is the slope for language positive?
```{r}
hypothesis(E3_mdl_reg, 'language1 > 0')
```

Is the slope for direction positive?
```{r}
hypothesis(E3_mdl_reg, 'direction1 > 0')
```

Is the interaction slope negative?
```{r}
hypothesis(E3_mdl_reg, 'direction1:language1 < 0')
```

Is the effect of language stronger than that of direction?
```{r}
hypothesis(E3_mdl_reg, 'language1 > direction1')
```

Is the effect of the interaction stronger than either of the main effects?
```{r}
hypothesis(E3_mdl_reg, 'direction1:language1 > direction1')
hypothesis(E3_mdl_reg, 'direction1:language1 > language1')
```


### Posterior predictive checks

Check posterior predictive simulations:
```{r}
pp_check(E3_mdl_reg, ndraws = 100)
```


# Comparing Frequentist and Bayesian approaches

## Advantages of Bayesian models
- ability to incorporate prior information
  - if strong previous evidence
  - improved model convergence, even with complex random effect structures, if regularising priors are used
- more flexible model specification (different distributions, can specify if distributions are truncated)
- generate estimates and credible intervals -> better able to discuss uncertainty of parameters and predictions
  - credible intervals are easier to interpret/more intuitive than confidence intervals (and p-values)
  - allow us to draw fine-grained conclusions rather than being forced into a binary decision (p < 0.05 or not? Confidence interval crosses 0 or not?)
- credible intervals not dependent on large-N approximations
- able to quantify support for or against null hypothesis or custom hypotheses  

## Issues with Bayesian models
- require learning new concepts and tools
- arguably more intuitive to interpret, but harder to fit
- take more time and effort
- prior selection can strongly influence model estimates and Bayes factors, especially with small Ns

