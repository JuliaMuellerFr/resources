---
title: "Text (and sentiment) analysis with R"
author: "Kyla McConnell & Julia MÃ¼ller"
date: "7/01/2020"
output: 
  html_document:
    theme: lumen
    toc: true
    toc_float: 
      collapsed: false
    toc_depth: 4
---


# Text Analysis with R 
Primary source: Text Mining with R, Julia Silge
https://www.tidytextmining.com/

```{r}
library(tidyverse) #for various data manipulation tasks
library(tidytext) #for text mining specifically, main package in book
library(stringr) #for various text operations
library(gutenbergr) #to access full-text books that are in the public domain
library(scales) # for visualising percentages
library(readtext) # for reading in txt files
library(wordcloud) # for creating wordclouds
library(textdata) #for afinn sentiment score dict
library(magrittr) #for different types of pipes
library(reshape2) #for data manipulation as matrices
library(wordcloud) #for wordclouds
library(sentimentr) #for sentence-based sentiment scores
library(viridis) #optional - provides colourblind-friendly colours
```


## 1 Reading in texts

### 1.1 txt files
Here's how you can read in one .txt file that is saved in the same location as this script (i.e. in the same folder on your computer):
```{r}
hf <- readtext("Adventures of Huckleberry Finn.txt")
```

If you want to read all files from a sub-folder, type the name of the folder followed by / and * to ask R to read in all files in that folder:
```{r}
shakes <- readtext("Shakespeare txts/*")
shakes$doc_id <- sub(".txt", "", shakes$doc_id) # this gets rid of .txt in the play titles
```


### 1.2 Book data from Project Gutenberg
- Project Gutenberg: free downloads of books in the public domain (i.e. lots of classic literature)
- Currently in a legal battle in Germany -* impossible to download via the website
- Still accessible via the R package gutenbergr by ID
- Top 100 books for inspiration (changes daily based on demand): https://www.gutenberg.org/browse/scores/top
- Catalog: https://www.gutenberg.org/catalog/ 

To find the id of a book (some have multiple copies):
```{r}
gutenberg_metadata %>%
  filter(title %in% c("Alice's Adventures in Wonderland", "Grimms' Fairy Tales", "Andersen's Fairy Tales"))
```

Can also search by author name:
```{r}
gutenberg_works(author == "Carroll, Lewis")
gutenberg_works(str_detect(author, "Carroll")) #if you only have a partial name
```

For more Gutenberg search options: https://ropensci.org/tutorials/gutenbergr_tutorial/

Once you found your books, download with gutenberg_download:
```{r}
fairytales_raw <- gutenberg_download(c(11, 2591, 1597))
fairytales_raw
```


### 1.3 Preparing data
- converting Gutenberg ID to a factor and replacing the ID numbers with more descriptive labels 
```{r}
fairytales_raw$gutenberg_id <- as.factor(fairytales_raw$gutenberg_id)
fairytales_raw$gutenberg_id <- plyr::revalue(fairytales_raw$gutenberg_id,
                                              c("11" = "Alice's Adventures in Wonderland",
                                                "2591" = "Grimm's Fairytales",
                                                "1597" = "Hans Christian Anderson's Fairytales"))
```


## 2 Tidy text
- One word per row, facilitates analysis
- Token: "a meaningful unit of text, most often a word, that we are interested in using for further analysis"

### 2.1 the unnest_tokens function
- Easy to convert from full text to token per row with unnest_tokens()
Syntax: unnest_tokens(df, newcol, oldcol)
- unnest_tokens() automatically removes punctuation and converts to lowercase (unless you set to_lower = FALSE)
- by default, tokens are set to words, but you can also use token = "characters", "ngrams", "sentences", "lines", "regex", "paragraphs", and even "tweets" (which will retain usernames, hashtags, and URLs)
```{r}
fairytales_tidy <- fairytales_raw %>% 
  unnest_tokens(word, text)
# this keeps the information on which sentence the words came from
fairytales_raw %>% 
  unnest_tokens(sentence, text, token = "sentences") %>% 
  mutate(sent_nr = row_number()) %>% 
  unnest_tokens(word, sentence)
fairytales_tidy
shakes_unnest <- shakes %>% 
  unnest_tokens(word, text)
```


### 2.2 Removing non-alphanumeric characters
- Project Gutenberg data sometimes contains underscores to indicate italics
- str_extract is used to get rid of non-alphanumeric characters (because we don't want to count _word_ separately from word)
```{r}
fairytales_tidy <- fairytales_tidy %>% 
  mutate(word = str_extract(word, "[a-z']+"))
shakes_unnest <- shakes_unnest %>% 
  mutate(word = str_extract(word, "[a-z']+"))
``` 


### 2.3 Stop words
- Stop words: very common, "meaningless" function words like "the", "of" and "to" -- not usually important in an analysis (i.e. to find out that the most common word in two books you are comparing is "the")
- tidytext has a built-in df called stop_words for English 
- remove these from your dataset with anti_join

We can take a look:
```{r}
stop_words
```

```{r}
fairytales_tidy <- fairytales_tidy %>% 
  anti_join(stop_words)
fairytales_tidy
```

Define other stop words:
```{r}
meaningless_words <- tibble(word = c("von", "der", "thy", "thee", "thou"))
fairytales_tidy <- fairytales_tidy %>% 
  anti_join(meaningless_words)
```
This could also be used to remove character names, for example.

The stopwords package also contains lists of stopwords for other languages, so to get a list of German stopwords, you could use:
```{r}
library(stopwords)
stop_german <- data.frame(word = stopwords::stopwords("de"), stringsAsFactors = FALSE)
```
More info: https://cran.r-project.org/web/packages/stopwords/readme/README.html


## 3 Analysing frequencies

### 3.1 Find most frequent words
- Easily find frequent words using count() 
- Data must be in tidy format (one token per line)
- sort = TRUE to show the most frequent words first

tidy_books %>%
  count(word, sort = TRUE) 

```{r}
fairytales_freq <- fairytales_tidy %>% 
  group_by(gutenberg_id) %>% #including this ensures that the counts are by book and the id column is retained
  count(word, sort=TRUE)
fairytales_freq
shakes_freq <- shakes_unnest %>% 
  group_by(doc_id) %>% 
  count(word, sort = TRUE)
```

Reminder: filter can be used to look at subsets of the data, i.e. one book, all words with freq above 100, etc. (Note here that I don't save this output)
```{r}
fairytales_tidy %>% 
  group_by(gutenberg_id) %>% 
  count(word, sort=TRUE) %>% 
  filter(gutenberg_id == "Grimm's Fairytales")
```

#### Plotting word frequencies - bar graphs

Bar graph of top words in Grimm's fairytales.

Basic graph:
```{r}
fairytales_freq %>% 
  filter(n>90 & gutenberg_id == "Grimm's Fairytales") %>% 
  ggplot(aes(x=word, y=n)) +
  geom_col()
```

Readable labels:
```{r}
fairytales_freq %>% 
  filter(n>90 & gutenberg_id == "Grimm's Fairytales") %>% 
  ggplot(aes(x=word, y=n)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 45))
```

Descending order:
```{r}
fairytales_freq %>% 
  filter(n>90 & gutenberg_id == "Grimm's Fairytales") %>% 
  ggplot(aes(x=reorder(word, -n), y=n)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 45))
```

Axis names and colors:
```{r}
fairytales_freq %>% 
  filter(n>90 & gutenberg_id == "Grimm's Fairytales") %>% 
  ggplot(aes(x=reorder(word, -n), y=n, fill=n)) +
  geom_col(show.legend=FALSE) +
  theme(axis.text.x = element_text(angle = 45)) +
  xlab("Word") +
  ylab("Frequency") +
  ggtitle("Most frequent words in Grimm's Fairytales")
```

Or: flip coordinate system to make more space for words
```{r}
fairytales_freq %>% 
  filter(n>90 & gutenberg_id == "Grimm's Fairytales") %>% 
  ggplot(aes(x=reorder(word, n), y=n, fill=n)) +
  geom_col(show.legend=FALSE) +
  xlab("Word") +
  ylab("Frequency") +
  ggtitle("Most frequent words in Grimm's Fairytales") +
  coord_flip()
```

### 3.2 Normalised frequency
- when comparing the frequencies of words from different texts, they are commonly normalised
- convention in corpus linguistics: report the frequency per 1 million words
- for shorter texts: per 10,000 or per 100,000 words
- calculation: raw frequency * 1,000,000 / total numbers in text
```{r}
# see the total number of words per play (doc_id)
shakes_freq %>% 
  group_by(doc_id) %>% 
  mutate(sum(n)) %>% 
  distinct(doc_id, sum(n))
shakes_freq <- shakes_freq %>% 
  na.omit() %>% 
  group_by(doc_id) %>% 
  mutate(pmw = n*1000000/sum(n)) %>% # creates a new column called pmw
  ungroup() %>% 
  anti_join(stop_words) # removing stopwords afterwards
shakes_freq %>% select(word, pmw)
```

#### Plotting normalised frequency
Now we can plot, for example, the 20 most frequent words (by pmw).
```{r}
shakes_freq %>% 
  filter(doc_id == "Othello") %>% 
  top_n(20, pmw) %>% 
  ggplot(aes(x=reorder(word, -pmw), y=pmw, fill=pmw)) +
  geom_col(show.legend=FALSE) +
  theme(axis.text.x = element_text(angle = 45)) +
  xlab("Word") +
  ylab("Frequency per 1 million words") +
  ggtitle("Most frequent words in Othello")
```


## 4 Comparing the vocabulary of texts

Next, we'll create two graphs to compare the vocabulary of our texts. First, we focus on Alice's Adventures and Anderson's Fairytales. The newly created comp_2 data frame contains only the words and their frequencies in the two texts in two separate columns.

### Comparing two texts
```{r}
comp_2 <- fairytales_freq %>% 
  filter(gutenberg_id == "Alice's Adventures in Wonderland"|gutenberg_id == "Hans Christian Anderson's Fairytales") %>% 
  group_by(gutenberg_id) %>% 
  mutate(proportion = n / sum(n)) %>% #creates proportion column (word frequency divided by overall frequency per author)
  select(-n) %>%
  spread(gutenberg_id, proportion)
head(comp_2)
```

Now, we can plot the words. Their placement depends on the word frequencies. Additionally, colour coding shows how different the frequencies are - darker items are more similar in terms of their frequencies, lighter-coloured ones more frequent in one text compared to the other. We'll discuss the interpretation in more detail once we've created the threeway comparison.
```{r}
ggplot(comp_2, 
       aes(x = `Alice's Adventures in Wonderland`, y = `Hans Christian Anderson's Fairytales`, 
           color = abs(`Alice's Adventures in Wonderland` - `Hans Christian Anderson's Fairytales`))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  theme_light() +
  theme(legend.position="none") +
  labs(y = "Hans Christian Anderson's Fairytales", x = "Alice's Adventures in Wonderland")
```

### Comparing three texts
In order to compare three texts, we need to add one step to the data preparation: Grimm's Fairytales will be the text that the other two will be compared to, so its word frequencies will be contained in the Grimm's Fairytales column. The gutenberg_id column contains both Alice's Adventures and Anderson's Fairytales so we can pass this column to the facet_wrap command and create two graphs.
```{r}
comp_3 <- fairytales_freq %>% 
  group_by(gutenberg_id) %>% 
  mutate(proportion = n / sum(n)) %>% #creates proportion column (word frequency divided by overall frequency per author)
  select(-n) %>% 
  spread(gutenberg_id, proportion) %>% 
  gather(gutenberg_id, proportion, "Alice's Adventures in Wonderland":"Hans Christian Anderson's Fairytales") # only done for plotting
head(comp_3); unique(comp_3$gutenberg_id)
```

The ggplot command is very similar to the one used above but facet_wrap is added to create two comparisons - Grimm's Fairytales compared to Alice's Adventures (left graph) and Grimm's compared to Anderson's fairytales (right graph):
```{r}
ggplot(comp_3, 
       aes(x = proportion, y = `Grimm's Fairytales`, 
           color = abs(`Grimm's Fairytales` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  theme_light() +
  facet_wrap(~ gutenberg_id, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Grimm's Fairytales", x = NULL)
```

### Interpretation
Words that are close to the diagonal line have similar frequencies in both texts (e.g. king, cook, calling in the left graph). Words that are far from the line are more frequent in one of the two texts (e.g. wife or son are more frequent for Grimm compared to Alice, while turtle and hare are more frequent in Alice than in Grimm). Again, this is also indicated by the colour.  
Generally, if the words are closer to the line and there's a smaller gap for low frequencies, the vocabulary of the texts overall is more similar. In our case, the two fairytale sources contain more of the same words than Grimm compared to Alice.  
An additional step in an analysis of word frequencies and something we won't cover today is to calculate correlations of word frequencies to quantify how similar the vocabularies of texts are.  


## 5 Word and document frequencies

### tf-idf
How can we quantify what a text/document is about? We could analyse the term frequency (tf) - how often does a term occur in a text/document. However, common words are the same in most texts, e.g. grammatical words like articles. A solution would be to instead analyse the inverse document frequency (idf) which lowers the importance of frequent words and raises the importance of rare words in documents. So it's a measure of how important a word is to a text compared to how important it is in the collection of texts.

#### The bind_tf_idf-function
- input: format needs to be one row per token (term), per document
- one column (here: word) contains the terms/tokens
- one column (here: gutenberg_id) contains the documents
```{r}
fairytales_idf <- fairytales_freq %>% 
  bind_tf_idf(word, gutenberg_id, n)
fairytales_idf %>%
  select(gutenberg_id, word, tf_idf) %>% 
  arrange(desc(tf_idf))
```
**interpretation:**
- low tf_idf if words appear in many books, high if they occur in few books  
- characteristic words for documents  
- so unsurprisingly, in our data, the first hits with the highest tf_idf are character names  

#### Characteristic words per book
visualisation of the top 20 tf-idf words per book:
```{r}
fairytales_idf$word <- as.factor(fairytales_idf$word)
fairytales_idf %>%
  group_by(gutenberg_id) %>% 
  arrange(desc(tf_idf)) %>% 
  top_n(20, tf_idf) %>% 
  ggplot(aes(x = reorder(word, tf_idf), y = tf_idf, fill = gutenberg_id)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~gutenberg_id, scales = "free") +
  coord_flip()
```

#### Characteristic words per chapter
tf_idf can also be used to find characteristic words per chapter, or any other text unit. We'll only use "Alice in Wonderland" as an example since it consists of several chapters.  
We first need to create a column that contains the chapter the word came from. The code below:

- finds "chapter" followed by a Roman numeral by using a regular expression regex("^chapter [\\divclx]" in the str_detect() command  
- extracts the chapter number by counting how often this regex is found with cumsum(). This is basically a counter that starts at 0 if the regex isn't matched, then counts up by one every time chapter + Roman numeral is found in the text  
- write it to a new column called "chapter"  
- also preserves the original line numbers (optional)  
We then remove the gutenberg_id column, words from chapter 0, i.e. the title and information on the edition, unnest tokens, and remove stopwords.  
```{r}
alice <- fairytales_raw %>% 
  filter(gutenberg_id == "Alice's Adventures in Wonderland") %>% 
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divclx]",
                                                 ignore_case = TRUE)))) %>%
  select(-gutenberg_id) %>% 
  filter(chapter != 0) %>% 
  mutate(chapter = as_factor(chapter)) %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words)
head(alice); summary(alice$chapter)
```

Now let's calculate the tf-idf per chapter:

- first step is to calculate the frequency for each word per chapter
- then apply bind_tf_idf function
- show words with the highest tf_idf
```{r}
alice <- alice %>% 
  group_by(chapter) %>% 
  count(word, sort = TRUE)
alice_idf <- alice %>% 
  bind_tf_idf(word, chapter, n)
alice_idf %>%
  select(chapter, word, tf_idf) %>% 
  arrange(desc(tf_idf))
```

Again, we can visualise the most characteristic words, this time per chapter:
```{r}
alice_idf %>%
  group_by(chapter) %>% 
  arrange(desc(tf_idf)) %>% 
  top_n(5, tf_idf) %>% 
  ggplot(aes(reorder(word, tf_idf), tf_idf, fill = chapter)) +
  geom_col(show.legend = FALSE) +
  theme_light() +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~ chapter, scales = "free") +
  coord_flip()
```



# Sentiment analysis 

## 1 Sentiment analysis based on single words

### 1.1 Animal Crossing Data

Tidy Tuesday Week 19: https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-05-05/readme.md

Animal Crossing is a 2020 "sandbox" game, where your character lives on an island with a variety of different animal characters and collects resources to progress and upgrade the island. It has had mixed reviews: either it is the best game ever, or boring and pointless. It has also been criticized for the fact that you can only have one save game per console ("forcing" families/couples to buy extra consoles to avoid fights over island decor..)

"user_reviews" includes the date of a review posting, the user_name of the writer, the grade they give the game (0-10), and the text they wrote.

```{r}
user_reviews_raw <- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv') #download from tidytuesday github
head(user_reviews_raw)
```

### 1.2 Preprocessing

- tokenisation: unnest_tokens
- remove stop words
  - not really necessary to remove all stop words for sentiment analysis -- if a word has no emotional/sentiment-based meaning, it will just receive no score. But can be useful for misclassified words.

```{r}
more_stop_words <- tibble(word = c("nintendo", "switch"))

user_reviews <- user_reviews_raw %>% 
  unnest_tokens(word, text) %>% 
  #anti_join(stop_words) %>% 
  anti_join(more_stop_words)

head(user_reviews)
```

### 1.3 Adding sentiment scores
- Assigns each word a score or a value based on entries in a pre-defined dictionary -- then adds up all the scores to get a score per textual unit
- Dictionaries created by crowd-sourcing (Amazon Mechanical Turk, Twitter data) and/or by work on parts of the author(s) in collecting and analyzing the words
- Major disadvantage: what if the text includes the phrase "not good"? Word-based scores will just see "good" and give a positive score!
- We'll look at three different dicts, all included in the tidytext package: 

- All dictionaries are called with get_sentiments()
- Then, join with inner_join (keeps all words that are in BOTH dataframes)
- And count or sum!

#### bing
- Binary: positive/negative

```{r}
head(get_sentiments("bing"))
```

Inner join to get a sentiment for each word:
```{r}
user_reviews %>% 
  inner_join(get_sentiments("bing")) %>% 
  head()
```

```{r}
user_reviews %>% 
  inner_join(get_sentiments("bing"))%>% 
  count(sentiment)
```

Most common positive/negative words? Count by word and sentiment
```{r}
user_reviews %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  head()
```

Each reviewer's sentiment score:
(Note: retain "grade" in count command to keep this column for later)
```{r}
user_reviews %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(user_name, grade, sentiment) %>% 
  head()
```

Create columns for negative and positive, add total_score column (& save)
```{r}
(user_sentiments_bing <- user_reviews %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(user_name, grade, sentiment) %>% 
  spread(sentiment, n, fill=0) %>% 
  mutate(total_score = positive - negative))
```

Look at the range of the scores
```{r}
user_sentiments_bing %>% 
  summarize(max(total_score), min(total_score)) 
```

Graph it:
```{r}
ggplot(aes(x=total_score, y=grade, group=grade), data=user_sentiments_bing) +
    geom_violin(aes(color=grade), show.legend=FALSE)
```

#### AFINN
- Scale: -5 (very negative) to 5 (very positive)
```{r}
head(get_sentiments("afinn"))
```

Join to reviews and take a look
```{r}
user_reviews %>% 
  inner_join(get_sentiments("afinn")) %>% 
  head()
```

Sum up one total per review (& save)
```{r}
(user_sentiments_afinn <- user_reviews %>% 
  group_by(user_name, grade) %>% #retain grade for use later
  inner_join(get_sentiments("afinn")) %>% 
  summarize(total_score = sum(value)) %>% 
  ungroup())
```

Look at the range of the scores
```{r}
user_sentiments_afinn %>% 
  summarize(max(total_score), min(total_score)) 
```

Graph it:
```{r}
ggplot(aes(x=total_score, y=grade, group=grade), data = user_sentiments_afinn) +
    geom_violin(aes(color=grade), show.legend=FALSE)
#Or with overlaid boxplots
ggplot(aes(x=total_score, y=grade, group=grade), data = user_sentiments_afinn) +
    geom_violin(aes(color=grade), show.legend=FALSE) +
  geom_boxplot(aes(color = grade), alpha = 0.2, width = 0.25, show.legend=FALSE) +
  theme_light() +
  coord_flip()
```

What's going on with the very negative sentiment scores for some positive reviews? Let's join the text back to the sentiment scores and take a look:
```{r}
user_sentiments_afinn <- left_join(user_sentiments_afinn, user_reviews_raw)
user_sentiments_afinn %>% 
  filter(grade > 8 & total_score < 0) %>% 
  select(grade, total_score, text) %>% 
  arrange(total_score)
```
Turns out these reviewers give 10 points to offset the (as they see it) unjustly low reviews while making fun of people who give 0 points or rate the game before it has come out.

Reversely, why positive scores with low grades?
```{r}
user_sentiments_afinn %>% 
  filter(grade < 3 & total_score > 20) %>% 
  select(grade, total_score, text) %>% 
  arrange(desc(total_score))
```
These reviewers tend to say that they initially liked the game until they realised that only one person could play it.

Compare the two methods so far:
```{r}
user_sentiments_bing <- user_sentiments_bing %>% 
  mutate(method="bing")
user_sentiments_afinn <- user_sentiments_afinn %>% 
  mutate(method="afinn")
bind_rows(user_sentiments_bing, 
          user_sentiments_afinn) %>%
  ggplot(aes(x = user_name, y = total_score, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y") +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```

#### nrc
- Multiple emotions: positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise
- Scale: binary: either assigned the emotion or not

```{r}
head(get_sentiments("nrc"))
```

Join sentiment scores and count, checking out informative words:
```{r}
user_reviews %>% 
  inner_join(get_sentiments("nrc")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  ungroup()
```

See: player, console -- this would be a case for our custom stop words.
Should be done at the beginning but let's do it now: 
```{r}
even_more_stop_words <- tibble(word = c("player", "console"))
user_reviews <- user_reviews %>% 
  anti_join(even_more_stop_words) 
user_reviews%>% 
  inner_join(get_sentiments("nrc")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  ungroup()
```

Since these are binary categories, we again use count and spread:
```{r}
user_reviews %>% 
  inner_join(get_sentiments("nrc")) %>% 
  count(user_name, grade, sentiment, sort = TRUE)%>% 
  spread(sentiment, n, fill=0)
```

Graph how many words of each sentiment show up by review grade:
Note: Same code as above, without spread() and without counting by user_name
```{r}
user_reviews %>% 
  inner_join(get_sentiments("nrc")) %>% 
  count(grade, sentiment, sort = TRUE) %>% 
  ggplot(aes(x=as.factor(grade), y=n, fill=sentiment)) +
  geom_col(position="dodge")
```

Reviews seem pretty polarized, let's try to normalize them by total sentiments expressed
```{r}
user_reviews %>% 
  inner_join(get_sentiments("nrc")) %>% 
  count(grade, sentiment, sort = TRUE) %>% 
  spread(sentiment, n, fill=0) %>%
  mutate(total = anger + anticipation + disgust + fear + joy + negative + positive + sadness + surprise + trust, 
         anger = round(anger / total, 2),
         anticipation = round(anticipation / total, 2),
         disgust = round(disgust / total, 2), 
         fear = round(fear / total, 2), 
         joy = round(joy / total, 2), 
         negative = round(negative / total, 2), 
         positive = round(positive / total, 2), 
         sadness = round(sadness / total, 2), 
         surprise = round(surprise / total, 2),
         trust = round(trust / total, 2))
```


### 1.4 Comparison clouds

Comparison clouds are extensions of word clouds. Instead of plotting the most frequent words in a text, with more frequent words in a bigger font, comparison clouds show word frequencies across documents. This means you can compare the vocabulary of texts, but we can also use it to look at the most frequent word per emotion, for example.
The input format for a comparison cloud needs to be a matrix, with the words in rows and one document per column. We can use the acast function from reshape2 to achieve this format.

In the comparison.cloud call, we can specify arguments such as the colours, the maximum number of words that should be plotted, how many of them should be rotated, etc. See ?comparison.cloud for more details.

```{r, warning=FALSE, message=FALSE}
user_reviews %>% 
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE)%>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#CA5336", "#009D93"),
                   max.words = 100,
                   title.size = 2, match.colors = TRUE)
user_reviews %>% 
  inner_join(get_sentiments("nrc")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(max.words = 100,
                   title.size = 1, match.colors = TRUE)
```

For reference, here's a wordcloud to visualise the most frequent words in the Shakespeare data. Here, the size indicates the frequency, with words that occur more often being displayed in a larger font size, but this can also be used to visualise e.g. normalised frequency (pmw) or length or anything else you pass to the freq = part of the command.
```{r}
wordcloud(words = shakes_freq$word, freq = shakes_freq$n, 
          min.freq = 150, max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```


### 1.5 Recap
- Three options for sentiment dictionaries by word (bing, afinn, nrc)
- Word-based sentiment analysis doesn't consider negation, etc.
- Remove any incorrectly labeled or overly influential words with a custom stop word list
- Equalize scores based on number of words, reviews, sentiments
- Larger sample size = more likely to even out to 0
- Good for basic overview, but prone to many different types of errors -- always look closely at your data, both as a summary table and plot!


## 2 Sentiment analysis for sentences

So far, we've used a dictionary lookup to add sentiments for each individual word. This approach can run into problems. For example, "happy" would always receive a positive score, even though depending on the context (e.g. "not happy at all") that might not be appropriate.
Next, we'll calculate sentiment scores by sentence, using the [sentimentr package](https://cran.r-project.org/web/packages/sentimentr/readme/README.html). This package also considers things like negation ("not happy") and amplifiers ("really happy" is more positive than just "happy") as well as deamplifiers (e.g. "barely") and adversative conjunctions (e.g. "but") when calculating sentiment scores.

You can run these lines of code to see the items that are considered negators, amplifiers, deamplifiers, and adversative conjunctions.
```{r}
lexicon::hash_valence_shifters[y==1] # negators 
lexicon::hash_valence_shifters[y==2] # amplifiers 
lexicon::hash_valence_shifters[y==3] # deamplifiers 
lexicon::hash_valence_shifters[y==4] # adversative conjunctions 
```

Let's look at a simple example to compare the two approaches:
```{r}
examples <- data.frame(
  id = c("1", "2"),
  text = c("I was very happy.", "She was not happy.")
)
sentiment(get_sentences(examples$text))
examples %>% 
  unnest_tokens(word, text) %>% 
  inner_join(get_sentiments("bing"))
```


### 2.1 Polarity

#### ...by entire texts
First, we'll calculate an average sentiment score for each of the texts (i.e. entire plays) in our Shakespeare data. *Be warned - since this is a lot of data, this will take a little while...*
```{r}
shakes_sentiments <- shakes %>%
    mutate(sents = get_sentences(text)) %$% 
    sentiment_by(sents, doc_id)
```

##### Plotting
The first line of code creates a graph in which the plays are sorted alphabetically (because when we read in the data, the files were sorted alphabetically) while the second command sorts the plays from highest to lowest sentiment score (removing the - will sort from lowest to highest).
```{r}
ggplot(shakes_sentiments) + 
  aes(x = doc_id, y = ave_sentiment) + 
  geom_point()
ggplot(shakes_sentiments) + 
  aes(x = reorder(doc_id, -ave_sentiment), y = ave_sentiment) + 
  geom_point()
ggplot(shakes_sentiments) + 
  aes(x = reorder(doc_id, -ave_sentiment), y = ave_sentiment) + 
  geom_point() + 
  labs(x = "Shakespeare play", y = "average sentiment score") +
  theme_minimal()
```

Adding information on the kinds of plays and creating a lollipop plot:
```{r}
shakes_sentiments <- shakes_sentiments %>% 
  mutate(type_of_play = if_else(doc_id %in% c("Midsummer", "Tempest", "Merchant of Venice", "As You Like It", "Shrew"), "comedy",
                                if_else(doc_id %in% c("King Lear", "Romeo and Juliet", "Macbeth", "Hamlet", "Julius Caesar", "Othello"), "tragedy", "history play")))
ggplot(shakes_sentiments) + 
  aes(x = reorder(doc_id, -ave_sentiment), y = ave_sentiment) + 
  geom_point(aes(colour = type_of_play), size = 4) +
  geom_segment(aes(x = doc_id, xend = doc_id, 
                   y = 0.05, yend = ave_sentiment,
                   colour = type_of_play), 
               size = 1, alpha = 0.6) +
  theme_minimal() +
  coord_flip() +
  labs(x = "Shakespeare play", 
       y = "average sentiment score",
       title = "Average sentiment scores by Shakespeare plays",
       subtitle = "Sentiment scores can distinguish tragedies from comedies")
```
So we can see that the sentiment scores accurately distinguish the comedies and tragedies, with the comedies receiving higher sentiment scores.


#### ...by sentences

For this next part, we'll use *Alice's Adventures in Wonderland* which we'll download from Project Gutenberg using the gutenbergr package. As explained above, we use str_detect in combination with a regular expression to find "chapter" followed by a Roman numeral. The cumsum command counts up by one every time this regular expression is matched. We then remove the Gutenberg ID and "chapter 0" (i.e. the title and publisher).
```{r}
alice_raw <- gutenberg_download(11, mirror = "http://mirrors.xmission.com/gutenberg/")

alice <- alice_raw %>% 
  mutate(chapter = cumsum(str_detect(text, regex("^chapter [\\divclx]", ignore_case = TRUE)))) %>%
  select(-gutenberg_id) %>% 
  filter(chapter != 0)
alice$chapter <- as_factor(alice$chapter)
```

Since the gutenbergr package preserves the line structure of the original text, we need to first paste the lines to form a coherent text. Otherwise, the get_sentences function won't work properly. 
```{r}
alice <- alice %>% 
  group_by(chapter) %>% 
  summarise(text_complete = paste(text, collapse=" ")) %>% 
  ungroup()
```

Let's now get sentiment scores for each sentence:
```{r}
alice_sentences <- alice %>% 
  get_sentences() %>% 
  sentiment()
```

...plot their development throughout the text:
```{r}
ggplot(alice_sentences) + 
  aes(sentence_id, sentiment) + 
  geom_smooth() + 
  geom_hline(yintercept=0, color = "red") +
  labs(x = "sentence", y = "average sentiment score",
       title = "Sentiment scores in Alice in Wonderland chapters") + 
  theme_minimal()
```

...and plot their development per chapter:
```{r}
ggplot(alice_sentences) + 
  aes(sentence_id, sentiment) + 
  geom_smooth() + 
  facet_wrap(~chapter, scales = "free_x") + 
  geom_hline(yintercept=0, color = "red") +
  labs(x = "sentence", y = "average sentiment score",
       title = "Sentiment scores in Alice in Wonderland chapters") + 
  theme_minimal()
```

We could also use sentiment_by to get sentiment scores by chapter, but it's often more flexible to work with the sentence scores and use group_by and summarise to calculate averages per chapter:
```{r}
alice_sentences %>% 
  group_by(chapter) %>% 
  summarise(sentiment_ch = mean(sentiment), sentiment_sd = sd(sentiment))
```

Finally, let's take a closer look at which sentences are more positive and which ones are more negative using highlight():
```{r}
alice %>% 
  filter(chapter %in% c("1", "11")) %>% 
  mutate(sents = get_sentences(text_complete)) %$% 
    sentiment_by(sents, chapter) %>% 
  highlight()
```


### 2.2 Emotions

So far, we've focused on polarity (positive - negative) per sentences, but the sentimentr package also provides many other interesting functions. Here are some we won't have time to go into today:  
- find and count instances of profanity
- replace emojis and internet slang/abbreviations with their text equivalents
- create your own sentiment dictionary

It's also possible to calculate ratings for the detailed emotions we discussed before. Instead of sentiment(_by), we use emotion(_by), e.g.
```{r}
alice_emotions <- alice %>% 
  get_sentences() %>% 
  emotion()
head(alice_emotions, 16) %>% select(emotion_type, emotion_count, emotion)
```

As an example, let's plot "fear" and "sadness" throughout the chapters:
```{r}
alice_emotions %>% 
  filter(emotion_type %in% c("fear", "sadness")) %>% 
  ggplot() + aes(sentence_id, emotion, fill = emotion_type, colour = emotion_type) +
  geom_smooth() + 
  facet_wrap(~chapter, scales = "free_x") + 
  labs(x = "sentence", y = "average sentiment score",
       title = "Sentiment scores in Alice in Wonderland chapters") + 
  theme_minimal()
```


## 3 Takeaway message
Keep in mind that no automated analysis can be perfect. Sentiment analysis has difficulties dealing with non-standard language such as informal spoken or historical registers. Always take a close look at your data!

Thorough data cleaning and annotation (e.g. finding a word's base form or 'lemma', in a process called 'lemmatisation') can improve its accuracy.
