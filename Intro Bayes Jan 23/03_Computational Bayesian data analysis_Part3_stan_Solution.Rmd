---
title: "03-Computational_Bayesian_data_analysis_Part3"
author: "Hanna Mahler"
date: "5 11 2021"
output: html_document
---

Today: Part 3 of "03 Computational Bayesian data analysis"
Not covered: Introduction to Stan
(Recommended: rewatching Part 2)

#0. Recap

Before we start, some revision questions:

1. Explain the following terms: conjugate case, sensitivity analysis.

- conjugate case: when we know the shape of both the prior and the posterior distribution so that we can describe them analytically
- sensitivity analysis: trying out different priors to see their effect on the model

2. What are the elements of a brms-formula that we need to specify for fitting a model?

- the predictors (up until now we only used rt ~ 1), the data, the likelihood function, the prior for each of the parameters, and for the sampling: the number of chains, iterations, and of the warmup-phase

3. What is the purpose of generating a prior predictive distribution? When do we do this?

  - What for: using only our priors (not the actual data) we are producing potential data sets. This helps us to see whether we need to change the priors or if they make sense.
  - When: while planning our experiments, can also happen before collecting actual data
  - Procedure: take a sample value of each prior, plug in these samples into the likelihood that we assume for our data, generate data set for sample value
  
4. What is the purpose of generating a posterior predictive distribution? When do we do this?

  - What for: using the fitted model (based on the priors, the likelihood, and the actual data) we are checking whether the model makes sense
  - When: after we have collected the data and fitted the model


#1. Prepare the environment

##1.1 Load libraries
```{r, warning = FALSE, message = FALSE}
options(scipen = 999)
set.seed(42)
library(MASS)
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(ggplot2)
library(brms)
#rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
library(bayesplot)
library(tictoc)
```

##1.2 Load data
Example 1: A single participant pressing a button repeatedly (simple linear model)
```{r}
df_noreading_data <- read_csv("button_press.csv")
df_noreading_data # data = reaction times in milliseconds
ggplot(df_noreading_data, aes(rt)) +
  geom_density() +
  ggtitle("Button-press data")
```

##1.3 Definde functions
```{r}
normal_predictive_distribution_fast <- function(mu_samples, sigma_samples, N_obs) {
  map2_dfr(mu_samples, sigma_samples, function(mu, sigma) {
    tibble(
      trialn = seq_len(N_obs),
      rt_pred = rnorm(N_obs, mu, sigma)
    )}, .id = "iter") %>%
    mutate(iter = as.numeric(iter))
}
## question: does this function only work if we assume a normal distribution with the values mu and sigma? Other distributions have other parameters that define them.
```

##1.4 Fit the model
```{r}
fit_press <- brm(rt ~ 1, 
                 data = df_noreading_data, 
                 family = gaussian(), 
                 prior = c(prior(uniform(0, 60000), class = Intercept), prior(uniform(0, 2000), class = sigma)), 
                 chains = 4, iter = 2000, warmup = 1000)

plot(fit_press)
```


#2. Posterior predictive distribution

Goal: assessing the quality of our model AFTER getting the data by predicting future data sets

Procedure:
- we take sample values of our parameters (mu and sigma) from the posterior distributions we created
- we create a new data set with these sample parameters
- we compare the actual data to the projected data

##2.1 Taking samples from the posterior

Same functions as for prior predictive distributions

```{r}
N_obs <- nrow(df_noreading_data)
mu_samples <- posterior_samples(fit_press)$b_Intercept
sigma_samples <- posterior_samples(fit_press)$sigma
(normal_predictive_distribution_fast(
  mu_samples = mu_samples,
  sigma_samples = sigma_samples,
  N_obs = N_obs
))
# result: a data frame with predicted reaction times
```

##2.2. Posterior predictive checks

Could the current data have been generated by our model?

```{r}
pp_check(fit_press, nsamples = 11, type = "hist") ## "pp_check" = posterior-predictive checks
# dark blue shows the actual distribution of the data
# light blue shows potential data sets predicted based on the give information (prior, likelihood, data)
pp_check(fit_press, nsamples = 100)
```

The graph shows that the predictions do not exactly correspond to our observed data. This is because we specify the model with a normal distribution despite our data being not normally distributed. 
-> potential solution: trying out different distributions, such as the log-normal likelihood (see below)

#3. Comparing different likelihoods

##3.1 The log-normal likelihood

Goal: Re-fitting the button-press example with log-normal likelihood instead of normal distribution.

  *Side Note: Why do we need log-transformations?* 
  Log-normal distributions are often more realistic as they are right-skewed and include only positive values 
  -> well suited for reaction times and reading times.
  Remember: our assumptions of the non-reading data (Video Part 1 11:15) were that
  1. there is a true underlying time it takes to press the space bar,
  2. that there is some noise in this process, 
  3. and that the noise is normally distributed (questionable, reaction times generally skewed) 


Previous assumption: Likelihood for each observation n as stemming from a normal distribution: rt<sub>n</sub> ~ Normal(ùúá, œÉ) [mean and range]

New assumption: Likelihood for each observation n as stemming from a log-normal distribution: rt<sub>n</sub> ~ LogNormal(ùúá, œÉ)
New parameters:
ùúá is the *location* of the log-transformed distribution (from which we can calculate the grand mean and grand median)
œÉ is the *scale* of the log-transformed distrubtion (= the standard deviation of the normal distribution of log(y))

  *Side note: What is log-transformation again?*
  Log-transformation is the opposite of the exponential value: exp(a) = b -> log(b) = a. 
  In our case: 
  log(y) ~ Normal(ùúá, œÉ)
  y ~ exp(Normal(ùúá, œÉ))
  See also: https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/PDF-log_normal_distributions.svg/450px-PDF-log_normal_distributions.svg.png 


##3.2 New prior predictive distribution based on log-transformed priors
```{r}
N_samples <- 1000
N_obs <- nrow(df_noreading_data)
mu_samples <- runif(N_samples, 0, 8)
sigma_samples <- runif(N_samples, 0, 1)

# Specifying the priors
# only difference: taking the exponential values here
prior_pred_ln <- exp(normal_predictive_distribution_fast(
  mu_samples = mu_samples, # = location
  sigma_samples = sigma_samples, # = scale
  N_obs
))

prior_pred_ln
```

##3.3 New distribution of statistics based on log-transformed priors

```{r}
(prior_pred_stat_ln <- 
   prior_pred_ln %>% ## using the transformed priors from above
   group_by(iter) %>%
   summarize(min_rt = min(rt_pred), 
             max_rt = max(rt_pred), 
             average_rt = mean(rt_pred), 
             median_rt = median(rt_pred)) %>%
   pivot_longer(cols = ends_with("rt"), 
                names_to = "stat", 
                values_to = "rt"))

## Plotting:
prior_pred_stat_ln %>% 
  ggplot(aes(rt)) + 
  scale_x_continuous("Reaction times in ms", 
                     trans = "log", breaks = c(0.001, 1, 100, 1000, 10000, 100000)) + 
  geom_histogram(colour = "white") + 
  facet_wrap(~stat, ncol = 1)
# careful, this graph shows log-scales, not the "real" data!
```

The priors are still not good, since they still come from a uniform distribution (but they are better than before)

##3.4 Choosing new, regularising priors for the log-transformed model

ùúá = Normal(6, 1.5)-> location
œÉ = Normal+(0, 1) -> scale

```{r}
#Median effect for our new priors:
c(lower = exp(6 - 2 * 1.5), higher = exp(6 + 2 * 1.5))
```

##3.5 New prior predictive distributions for log-transformed model with new, regularising priors 

and generating new statistics for it
```{r, warning = FALSE, message = FALSE}
library(MCMCglmm)
N_samples <- 1000
N_obs <- nrow(df_noreading_data)
mu_samples <- rnorm(N_samples, 6, 1.5)
sigma_samples <- rtnorm(n = N_samples, mean = 0, sd = 1) # this apparently requires the MCMCglmm package, and I can only get it to work when deleting his "a = 0"
(prior_pred_ln_better <- exp(normal_predictive_distribution_fast( ## exponentitation
  mu_samples = mu_samples,
  sigma_samples = sigma_samples,
  N_obs
)))

##
(prior_pred_stat_better_ln <- prior_pred_ln_better %>%
group_by(iter) %>%
    summarize(
      min_rt = min(rt_pred),
      max_rt = max(rt_pred),
      average_rt = mean(rt_pred),
      median_rt = median(rt_pred)
      ) %>%
    pivot_longer(
cols = ends_with("rt"),
names_to = "stat", values_to = "rt"
))

## plotting the statistics of the prior predictive distributions
prior_pred_stat_better_ln %>% ggplot(aes(rt)) + 
  scale_x_continuous(trans = "log", breaks = c(0.001, 1, 100, 1000, 10000, 100000)) + 
  geom_histogram(colour = "white") + 
  facet_wrap(~stat, ncol = 1) + 
  coord_cartesian(xlim = c(0.001, 300000))
```

These values are a lot better! :)

##3.6 Fitting a new model based on the log-transformed data with the new, regularising priors
```{r}
fit_press_ln <- brm(rt ~ 1,
                    data = df_noreading_data,
                    family = lognormal(), # new: lognormal-family
                    prior = c(
                      prior(normal(6, 1.5), class = Intercept),
                      prior(normal(0, 1), class = sigma)))

fit_press_ln ## careful: model output (Intercept etc.) is in log-scale! Not the "real" values!
plot(fit_press_ln)
```

##3.7 Transforming the model results back to the basic scale

How long does it take to press the space bar?
```{r, warning = FALSE}
## transforming the model results back with exp()
estimate_ms <- exp(posterior_samples(fit_press_ln)$b_Intercept)
c(mean = mean(estimate_ms), quantile(estimate_ms, probs = c(.025, .975)))
```

##3.8 Posterior predictive checks with the new log-transformed model (with new, regularising priors)

```{r}
pp_check(fit_press_ln, nsamples = 100)
```

##3.9 Comparing the log-transformed model to the Normal likelihood model

Minimum values
```{r}
pp_check(fit_press, type = "stat", stat = "min") + 
  ggtitle("Normal model")
pp_check(fit_press_ln, type = "stat", stat = "min") + 
  ggtitle("Log-normal model")
# blue line = actual observed minimum value in the data
```

Maximum values
```{r}
pp_check(fit_press, type = "stat", stat = "max") + 
  ggtitle("Normal model")
pp_check(fit_press_ln, type = "stat", stat = "max") + 
  ggtitle("Log-normal model")
# blue line = actual observed maximum value in the data
```

#4. Introduction to Stan

We are likely to only ever use the brms-package in R and not really have any contact with Stan itself. See slides for further info on how Stan works.

Potentially useful for us:
- library "bayesplot" with functions to plot samples (e.g. mcmc_dens())
- inspecting the stan code after fitting a model with  brms::stancode(modelname) or before fitting a model with brms::make_stancode(model formula)
- extracting the data after the model was run with brms::standata(modelname) or before the model was run with brms::make_standata(model formula)


#5. Exercises

##5.1 On posterior predictive distributions

1. Generate posterior predictive distributions based on the previous model (3.8.2.1) and plot them.

```{r}
## Previous (highly skewed) model: 
fit_press_skewed <- brm(rt ~ 1, # no predictors here, only intercept
                 data = df_noreading_data, 
                 family = gaussian(),
                 prior = c(
                   prior(normal(400, 10),class = Intercept),
                   prior(normal(10, 1), class = sigma)
                 ), chain = 4,
                 iter = 2000,
                 warmup = 1000)

plot(fit_press_skewed)

## Generate samples of the posterior:
N_obs <- nrow(df_noreading_data)
mu_samples <- posterior_samples(fit_press_skewed)$b_Intercept
sigma_samples <- posterior_samples(fit_press_skewed)$sigma
(normal_predictive_distribution_fast(
  mu_samples = mu_samples,
  sigma_samples = sigma_samples,
  N_obs = N_obs
))

## Plot the samples:
pp_check(fit_press_skewed, nsamples = 11, type = "hist")
pp_check(fit_press_skewed, nsamples = 100)
```


##5.2 On the model based on log-normal likelihood

1. For the log-normal model fit_press_ln, change the prior of ùúé so that it is a lognormal distribution with location (ùúá) of ‚àí2 and scale (ùúé) of 0.5. What is the meaning of this prior? Is it a good prior? Generate and plot prior predictive distributions. Do the new estimates change when you fit the model?

```{r}
## Specifying the new prior:
N_samples <- 1000
N_obs <- nrow(df_noreading_data)
mu_samples2 <- rlnorm(N_samples, 0, 60000) # samples from a uniform distribution
sigma_samples2 <- rlnorm(N_samples, 0, 2000)

## Prior predictive distributions with the new prior:
(prior_pred2 <- normal_predictive_distribution_fast(
  mu_samples = mu_samples2,
  sigma_samples = sigma_samples2,
  N_obs
))

## Plotting:
prior_pred2 %>%
  filter(iter <= 12) %>%
  ggplot(aes(rt_pred)) +
  geom_histogram() + 
  facet_wrap(~iter, ncol = 3)

## Checking the estimates of the model (using the new prior)
fit_press_ln2 <- brm(rt ~ 1,
                    data = df_noreading_data,
                    family = lognormal(), # new: lognormal-family
                    prior = c(
                      prior(lognormal(-2, 1.5), class = Intercept),
                      prior(lognormal(0.5, 1), class = sigma)))#

fit_press_ln2 ## careful: model output (Intercept etc.) is in log-scale! Not the "real" values!
plot(fit_press_ln2)

estimate_ms <- exp(posterior_samples(fit_press_ln)$b_Intercept)
c(mean = mean(estimate_ms), quantile(estimate_ms, probs = c(.025, .975)))

estimate_ms2 <- exp(posterior_samples(fit_press_ln2)$b_Intercept)
c(mean = mean(estimate_ms2), quantile(estimate_ms2, probs = c(.025, .975)))

```


2. For the log-normal model, what is the mean (rather than median) time that takes to press the space bar, what is the standard deviation of the reaction times in milliseconds?

```{r}
## mean time (in ms):

## standard deviation (in ms):

```


3. Would it make sense to use a ‚Äúskew normal distribution‚Äù instead of the lognormal? The skew normal distribution has three parameters location ùúâ, scale ùúî, and shape ùõº. The distribution is right skewed if ùõº > 0, is left skewed if ùõº < 0, and is identical to the regular normal distribution if ùõº = 0. For fitting this in brms, one needs to change family and set it to skew_normal(), and add a prior of class = alpha (location remains class = Intercept and scale, class = sigma).

‚Ä¢ Fit this model with a prior that assigns approximately 95% of the prior probability mass of alpha to be between 0 and 10.

```{r}

```

‚Ä¢ Generate posterior predictive distributions and compare the posterior distribution of summary statistics of the skew normal with the normal and log-normal

```{r}

```












