---
title: "03_Computational_Bayesian_data_analysis_21.10.2021"
author: "Sarah Warchhold"
date: "15 10 2021"
output: html_document
---
Exercises in Line 172 (part 1) and 440 (part 2)


#### 03 Computational Bayesian data analysis
from https://vasishth.github.io/bayescogsci/book/ch-compbda.html
and https://vasishth.github.io/IntroductionBDA/index.html

What we did in the video:
- running powerful linear&nonlinear models with brms
- deriving posterior (distribution) analytically is only possible for very few cases because denominator/marginal likelihood requires us to integrate the numerator 
- recap: marginal likelihood = also called evidence, prior predictive, partition function

- Alternative to deriving analytically: deriving posterior through sampling
Example: umbrella (80 times out of 100)
- Likelihood is binomial distribution (0.8)
- Prior for Œ∏ (theta): Beta(a=4, b=4) -> graph 0 to 1 would show: maximum in center, but not really spiked

### Computing the posterior (Remember from last time, just to have a look at it)
- Posterior = compromise prior & data
- Given a Binomial(n,k|theta) likelihood and a Beta(a, b) prior on theta, the posterior will be 

Beta(a + k, b + n - k)

k = successes; n = total number of trials; a & b = parameters of Beta distribution (selected prior)

Our data: 100 trials, 80 answers with umbrella

Posterior = Beta(a + 80, b + 100 - 80)
= Beta(a + 80, b + 20)

Plugging in a prior, e.g. Beta(4, 4)
Posterior = Beta(4 + 80, 4 + 20)
= Beta(84, 24)

```{r summary computing posterior}
theta <- seq(0, 1, by = 0.01)
# Prior:
plot(dbeta(theta, 2, 2), type = "l")

# Data:
plot(dbeta(theta, 80, 20), type = "l")

# Posterior:
plot(dbeta(theta, 84, 24), type = "l")
```
### 3.1 Deriving the posterior through sampling
For sampling from posterior distribution of theta:
- we use a probabilistic programming language
- given enough samples we will have a good approximation of the real posterior distribution 

Why Computational Bayesian data analysis?
- increase in computing power and appearance of probabilistic programming languages: impossible 20-30 years ago!
- easier alternatives based on Stan are rstanarm and brms

## Bayesian Regression Models using 'Stan': brms
- benefit: powerful, easy to use + produces Stan models on the fly and you can follow the procedure

```{r load packages setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#load the following
set.seed(42)
library(MASS)
## be careful to load dplyr after MASS
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(ggplot2)
library(brms)
library(rstan)
## Save compiled models:
rstan_options(auto_write = TRUE)
## Paralelize the chains using all the cores:
options(mc.cores = parallel::detectCores())
library(bayesplot)
library(cowplot)
library(tictoc)
```

## Example 1: A single participant presses space bar
Data: reaction times in milliseconds in each trial
Question: How long does it take to press a key when there is no decision involved?

Assumptions we need:
1. There is a true underlying time, ùúá, that the participant needs to press the space bar. 
2. There is some noise in this process. -> It does not always takes the same time
3. The noise is normally distributed (this assumption is questionable given that reaction times are generally skewed; we Ô¨Åx this assumption later).

Formal: Likelihood for each observation n: rt<sub>n</sub> ~ Normal (ùúá, œÉ)
Bad priors: 
ùúá~ Uniform (0, 60000) -> every value of ùúábetween 0 and 60seconds is equally likely
œÉ ~ Uniform(0,2000) (some range more likely than other range)

```{r first load data from bcogsci}
#loading data
library(bcogsci)
data("df_spacebar")
df_spacebar
```
# Plot it first
```{r ggplot}

ggplot(df_spacebar, aes(rt)) +
  geom_density() +
  xlab("response times")+
  ggtitle("Button-press data")
```
-> our assumption of normal distribution of reaction times is not very accurate: long tail, not symmetrical 

# Specifing the model in brms
```{r specifing}
fit_press <- brm(rt ~ 1, #fitting an intercept
  data = df_spacebar, #defining data
  family = gaussian(), # makes explicit that the    underlying likelihood function is a normal               distribution (Gaussian and normal are synonyms) that is implicit in lm
  prior = c(
    prior(uniform(0, 60000), class = Intercept),
    prior(uniform(0, 2000), class = sigma)
  ), # takes as argument a vector of priors. Although this specification of priors is optional, the    researcher should always explicitly specify each prior (not necessary when using lm?)
  chains = 4, # refers to the number of independent runs for sampling (by default four).
  iter = 2000, # refers to the number of iterations that the sampler makes to sample from the posterior           distribution of each parameter (by default 2000).
  warmup = 1000 # refers to the number of iterations from the start of sampling that are eventually           discarded (by default half of iter).
)  
```
Sampling and convergence in a nutshell: 
1. Chains start in random locations
2. in each interation they take one sample each
3. samples at the beginning do not belong to the posterior distribution
4. eventually, the chains end up in the vicinity of the posterior distribution
5. from that point onwards the samples will belong to the posterior

-> when the chains finally are mixing we can 'zoom in' and see the 'fat hairy caterpillar' (if nothing failed)

# Output of brms
```{r}
as_draws_df(fit_press) 
# b_Intercept in the brms output corresponds to our Œº, and lp is not really part of the posterior, it‚Äôs the density of the unnormalized log posterior for each iteration.
```
```{r}
plot(fit_press) 
#carterpillar, already thrown away warm-up; distribution usually look 'normal-like'
```

```{r}
fit_press #summary 
# posterior_summary(fit_press) is also useful
```
```{r}
# Estimate is just the mean of the posterior sample, and the CIs mark the lower and upper bounds of the 95% credible intervals:
posterior_samples(fit_press)$b_Intercept %>% 
  mean()
# 168.6369
posterior_samples(fit_press)$b_Intercept %>% 
 quantile(c(0.025, .975))
# 2.5%    97.5%
# 166.0876 171.1318 

#as_draws_df(fit_press)$b_Intercept %>% mean()
#as_draws_df(fit_press)$b_Intercept %>% 
  quantile(c(0.025, .975))
```
Since the fit was a success, ask yourself the following questions:

1. What information are the priors encoding? Do the priors make sense?
2. Does the likelihood assumed in the model make sense for the data?


## Exercise 3.1 A simple linear model.
a) Fit the model fit_press with just a few iterations, say 50 iterations. What happens?
```{r}

```

b) Using normal distributions, choose priors that represent better your assumptions about response times.
```{r}

```


### Video Part 2 + Text
# Important questions
1. What information are the priors encoding? Do the priors make sense? (see 3.2)
2. Does the likelihood assumed in the model make sense for the data?

### 3.2 Prior predictive distribution
Recap: We had defined the following priors for our linear model:
  Œº‚àºUniform(0,60000)
  œÉ‚àºUniform(0,2000)
These priors encode assumptions about the kind of data we would expect to see. To understand these assumptions we are generating data from the model.

prior predictive distribution = data, which is generated entirely by the prior distributions
We want to know if the priors do generate realistic-looking data?

Possible via sampling:
Repeat the following many times:
1. Take one sample from each of the priors.
2. Plug those samples into the probability density/mass function to generate a data set ypred1,‚Ä¶,ypredn
Each sample is an imaginary or potential data set.

```{r function for generating prior predictive distributions}
normal_predictive_distribution <- function(mu_samples, sigma_samples, N_obs) {
  # empty data frame with headers:
  df_pred <- tibble(
    trialn = numeric(0),
    rt_pred = numeric(0),
    iter = numeric(0)
  )
  # i iterates from 1 to the length of mu_samples, which we assume is identical to the length of the sigma_samples:
  for (i in seq_along(mu_samples)) {
    mu <- mu_samples[i]
    sigma <- sigma_samples[i]
    df_pred <- bind_rows(
      df_pred,
      tibble(
        trialn = seq_len(N_obs), # 1, 2,... N_obs
        rt_pred = rnorm(N_obs, mu, sigma),
        iter = i
      )
    )
  }
  df_pred
}

# produces 1000 samples of the priors predictive distribution
```
More efficient version of this function possible with purr (see Box 3.1 in the book/page)

Note: since brms still depends on Stan‚Äôs sampler, which uses Hamiltonian Monte Carlo, the prior sampling process can also fail to converge, especially when one uses very uninformative priors, as the ones in this example. In contrast, our function using rnorm cannot have convergence issues and will always produce independent samples.

```{r }
N_samples <- 1000
N_obs <- nrow(df_spacebar)
mu_samples <- runif(N_samples, 0, 60000)
sigma_samples <- runif(N_samples, 0, 2000)
tic()
prior_pred <- normal_predictive_distribution(
  mu_samples = mu_samples,
  sigma_samples = sigma_samples,
  N_obs = N_obs
)
toc()
```
```{r}
prior_pred
```

```{r Eighteen samples from the prior predictive distribution of the model defined in section 3.1.1.1.}
prior_pred %>%
  filter(iter <= 18) %>%
  ggplot(aes(rt_pred)) +
  geom_histogram() +
  facet_wrap(~iter, ncol = 3)
```

We know distributions are right skewed -> not shown here + reaction times odd
```{r Distribution of Statistics} 
(prior_stat <- prior_pred %>% 
   group_by(iter) %>% 
   summarize(min_rt = min(rt_pred),
             max_rt = max(rt_pred),
             average_rt = mean(rt_pred)) %>% 
   #we convert previous data frame to long one where min_rt, max_rt and average_rt are possible values of the columns 'stat'
   pivot_longer(cols = ends_with("rt"),
                names_to = "stat", 
                values_to = "rt"))
```
```{r}
prior_stat %>% 
  ggplot(aes(rt)) +
  geom_histogram(binwidth = 500) +
  facet_wrap (~stat, ncol=1)
#with more sample the graph will get more flat
```
Why are our distributions so bad?
- We used much less prior information than what we really had: our priors are clearly not very realistic given what we know about reaction times for such a button pressing task

What priors should we have chosen?

### 3.3 The influence of priors: sensitivity analysis
4 main classes/types of priors:
## 1. Flat, uninformative priors: priors as uninformative as possible
- choosing as uninformative priors as possible to 'let the data speak for itself'
- but: 
1) prior is as subjective as the likelihood (different choices of likelihood might have much stronger impact on posterior than different choices of priors)
2) uninformative priors are in general unrealistic (equal weight to any value)
3) uninformative priors make sampling slower, might lead to convergence problems
4) it is not always clear to which parametrization of a given distribution we should assign the flat priors
5) if we want to compute Bayes factors, uninformative priors can lead to very misleading conclusions

## 2. Regularizing priors: priors that downweight extreme values, not very informative, mostly letting the likelihood dominate in determining the posteriors
- also called weakly informative/mildly informative priors
- These are priors that downweight extreme values,  they are usually not very informative, and mostly let the likelihood dominate in determining the posteriors
- these priors are theory-neutral: they usually do not bias the parameters to values supported by any prior belief or theory
- idea: help stabilize computation

## 3. (prefered) Principled priors: priors that encode all (or most of) the theory-neutryl information that we do have
- idea is to have priors that encode all (or most of) the theory-neutral information that we do have
- we generally know what our data do and do not look like, we can build priors that truly reflect the properties of potential data sets, using prior predictive checks

## 4. Informative priors: cases, where we have a lot of prior knowledge and not much data
- There are cases where we have a lot of prior knowledge, and not much data. In general, unless we have very good reasons for having informative priors, we don‚Äôt want our priors to have too much influence on our posterior
- important, when we are investigating a language-impaired population from which we can‚Äôt get many subjects


### 3.4 Revisiting the button-pressing example with different priors

What would hapen if we use even wider priors for the model defined earlier?
```{r}
# We fit the model with the default setting of the sampler:
# 4 chains, 2000 iterations with half of them as warmup.
fit_press_unif <- brm(rt ~ 1,
  data = df_spacebar,
  family = gaussian(),
  prior = c(
    prior(uniform(-10^10, 10^10), class = Intercept),
    prior(uniform(0, 10^10), class = sigma)
  )
)
```
even with unrealistic prior, the output of the model is virtually identical to the previous one
```{r}
fit_press_unif
```
Using very informative priors:
  ‚àºNormal(400,10)
  œÉ‚àºNormal+(100,10) -> Normal+ indicates normal distribution truncated in zero that only allows positive values (no negative reaction times!)
```{r}
fit_press_inf <- brm(rt ~ 1,
  data = df_spacebar,
  family = gaussian(),
  prior = c(
    prior(normal(400, 10), class = Intercept),
    # brms knows that SDs need to be bounded 
    # to exclude values below zero:
    prior(normal(100, 10), class = sigma)
  )
)
```
```{r}
# the likelihood mostly dominates and the new estimates are just a couple of milliseconds away from our previous estimates:
fit_press_inf
```

Relatively principled priors:
experience from previous experiments showed mean reaction time around 200ms, but large degree of uncertainty
relatively wide prior, Normal(200,100)

```{r}
fit_press_reg <- brm(rt ~ 1,
  data = df_spacebar,
  family = gaussian(),
  prior = c(
    prior(normal(200, 100), class = Intercept),
    prior(normal(50, 50), class = sigma)
  )
)
```

```{r}
#estimates virtually the same as before
fit_press_reg
```

Note: The above examples of using different priors should not be misunderstood to mean that priors never matter. When there is enough data, the likelihood will dominate in determining the posterior distributions. What constitutes ‚Äúenough‚Äù data is also a function of the complexity of the model; as a general rule, more complex models require more data.

Cave: The posterior uncertainty of the population-level parameters will also be affected by the uncertainty in the group-level parameters. In other words, if the posterior estimates for the random effects have wide uncertainty, then the posteriors of the fixed effects will be affected by that uncertainty.

In order to determine the extent to which the posterior is influenced by our priors, it is a good practice to carry out a sensitivity analysis: 
we try different priors and either verify that the posterior doesn‚Äôt change drastically, or report how the posterior is affected by some specific priors (for examples from psycholinguistics, see Vasishth et al. 2013; Vasishth and Engelmann 2021).

### 3.5 Posterior predictive distribution

Prior predictive distribution = collection of data sets generated from the model (the likelihood and the priors)

->  now using the posterior distributions to generate future data from the model

```{r}
#as before, but instead of sampling mu and sigma from the priors, we use samples from the posterior
N_obs <- nrow(df_spacebar)
mu_samples <- as_draws_df(fit_press)$b_Intercept
sigma_samples <- as_draws_df(fit_press)$sigma
normal_predictive_distribution(
  mu_samples = mu_samples,
  sigma_samples = sigma_samples,
  N_obs = N_obs
)
```
We can use the posterior predictive distribution to examine the ‚Äúdescriptive adequacy‚Äù of our models.
Model comparison criteria (after Roberts and Pashler 2000) say:
- it is not enough to have a good fit to the data for a model to be convincing
- one should check that the range of predictions that the model makes is reasonably constrained; if a model can capture any possible outcome, then the model fit to a particular data set is not so informative. 
- Thus, although posterior predictive checking is important, it is only a sanity check to assess whether the model behavior is reasonable.

```{r pp_check}
#can show us different visualizations of posterior predictive checks
pp_check(fit_press, ndraws = 11, type = "hist")
```

```{r}
pp_check(fit_press, ndraws = 100, type = "dens_overlay")
```
This posterior predictive check shows a slight mismatch between the observed and predicted data.


What would happen if we use even wider priors for the model?
```{r}
fit_press_unif <- brm(rt~1,
                      data = df_spacebar,
                      family = gaussian(),
                      prior = c(
                        prior(uniform(-10^10, 10^10), class = Intercept), 
                        prior(uniform(0,10^10), class = sigma))
                      )
```
- Output of Model is virtually identical!

Informative priors, that are off:
```{r }
#not working? Should show, that values just a few miliseconds away
fit_press_inf <- brm(rt~1,
                     data = df_spacebar,
                     family = gaussian(),
                     prior = c(
                       prior(norma(400,10), class= Intercept),
                       #brms knows that SD need to be bounded by zero
                       prior(normal(100,10), class= sigma)
                     )
                     )
```

This doesn't mean priors never matter:
- When there is enough data for a certain parameter, the likelihood will dominate
- If we are not sure about the extent to which the posterior is influenced by our priors, we can do a sensitivity analysis
- We can use prior predictive distributions to see if we are on the right order of magnitude for our priors

## Exercise 3.2 Revisiting the button-pressing example with different priors.

a) Can you come up with very informative priors that bias the posterior in a noticeable way (use normal distributions for priors, not uniform priors)?
```{r}

```

b) Generate and plot prior predictive distributions based on this prior and plot them.
```{r}

```

c) Generate posterior predictive distributions based on this prior and plot them.
```{r}

```

## Exercise 3.3 Posterior predictive checks with a log-normal model.
a) For the log-normal model fit_press_ln, change the prior of œÉ so that it is a log-normal distribution with location (Œº) of ‚àí2 and scale (œÉ) of .5. 
What does such a prior imply about your belief regarding button-pressing times in milliseconds? Is it a good prior? Generate and plot prior predictive distributions. Do the new estimates change compared to earlier models when you fit the model?
```{r}

```

b) For the log-normal model, what is the mean (rather than median) time that takes to press the space bar, what is the standard deviation of the response times in milliseconds?
```{r}

```


## Exercise 3.4 A skew normal distribution.

Would it make sense to use a ‚Äúskew normal distribution‚Äù instead of the lognormal? The skew normal distribution has three parameters location Œæ, scale œâ, and shape Œ±. The distribution is right skewed if Œ±>0, is left skewed if Œ±<0, and is identical to the regular normal distribution if Œ±=0. For fitting this in brms, one needs to change family and set it to skew_normal(), and add a prior of class = alpha (location remains class = Intercept and scale, class = sigma).

a)  Fit this model with a prior that assigns approximately 95% of the prior probability mass of alpha to be between 0 and 10.
```{r}

```

b)  Generate posterior predictive distributions and compare the posterior distribution of summary statistics of the skew normal with the normal and log-normal
    
```{r}

```


## Comparing different likelihoods
- log-normal more realistic: only positive real values and is right-skewed

## The log-normal likelihood
If y is log-normally distributed, this means that log(y) is normally distributed.
We can obtain samples from the log-normal distribution, using the normal distribution by first setting an auxiliary variable, z, so that z=log(y).

```{r}
mu <- 6
sigma <- 0.5
N <- 500000
# Generate N random samples from a log-normal distribution
sl <- rlnorm(N, mu, sigma)
ggplot(tibble(samples = sl), aes(samples)) +
  geom_histogram(aes(y = ..density..), binwidth = 50) +
  ggtitle("Log-normal distribution\n") +
  coord_cartesian(xlim = c(0, 2000))
# Generate N random samples from a normal distribution,
# and then exponentiate them
sn <- exp(rnorm(N, mu, sigma))
ggplot(tibble(samples = sn), aes(samples)) +
  geom_histogram(aes(y = ..density..), binwidth = 50) +
  ggtitle("Exponentiated samples from\na normal distribution") +
  coord_cartesian( xlim = c(0, 2000))
```

## Re-fitting a single subject pressing a button repeatedly with a log-normal likelihood
If we assume that response times are log-normally distributed, we‚Äôll need to change our likelihood function as follows:
rtn‚àºLogNormal(Œº,œÉ)
-> parameters are in a different scale than the dependent variable!

```{r generating prior predictive distribution}
#We can just exponentiate the samples produced by normal_predictive_distribution() (or, alternatively, we could have edited the function and replaced rnorm with rlnorm).
N_samples <- 1000
N_obs <- nrow(df_spacebar)
mu_samples <- runif(N_samples, 0, 11)
sigma_samples <- runif(N_samples, 0, 1)
prior_pred_ln <- normal_predictive_distribution(
  mu_samples = mu_samples,
  sigma_samples = sigma_samples,
  N_obs = N_obs
) %>%
  mutate(rt_pred = exp(rt_pred))
```

Before generating the prior predictive distributions, we can calculate the values within which we are 95% sure that the expected median of the observations will lie. 
```{r}
# We do this by looking at what happens at two standard deviations away from the mean of the prior, Œº, that is 6‚àí2√ó1.5 and 6+2√ó1.5, and exponentiating these values:
c(
  lower = exp(6 - 2 * 1.5),
  higher = exp(6 + 2 * 1.5)
)

```
Using brms to generate prior predictive data: we need to have some simulated values that represent the vector of dependent variables, rt.
```{r}
df_spacebar_ref <- df_spacebar %>%
  mutate(rt = runif(n(), 0, 10000))
fit_prior_press_ln <- brm(rt ~ 1,
  data = df_spacebar_ref,
  family = lognormal(),
  prior = c(
    prior(normal(6, 1.5), class = Intercept),
    prior(normal(0, 1), class = sigma)
  ),
  sample_prior = "only",
  control = list(adapt_delta = .9)
)
```

```{r Plot the prior predictive distribution of means}
pp_check(fit_prior_press_ln, type = "stat", stat = "mean") +
  coord_cartesian(xlim = c(0.001, 300000)) +
  scale_x_continuous("Response times [ms]",
    trans = "log",
    breaks = c(0.001, 1, 100, 1000, 10000, 100000),
    labels = c(
      "0.001", "1", "100", "1000", "10000",
      "100000"
    )
  ) +
  ggtitle("Prior predictive distribution of means")
```
```{r plot the distribution of minimum, and maximum values}

p1 <- pp_check(fit_prior_press_ln, type = "stat", stat = "mean") +
  coord_cartesian(xlim = c(0.001, 300000)) +
  scale_x_continuous("Response times [ms]",
    trans = "log",
    breaks = c(0.001, 1, 100, 1000, 10000, 100000),
    labels = c(
      "0.001", "1", "100", "1000", "10000",
      "100000"
    )
  ) +
  ggtitle("Prior predictive distribution of means")
p2 <- pp_check(fit_prior_press_ln, type = "stat", stat = "min") +
  coord_cartesian(xlim = c(0.001, 300000)) +
  scale_x_continuous("Response times [ms]",
    trans = "log",
    breaks = c(0.001, 1, 100, 1000, 10000, 100000),
    labels = c(
      "0.001", "1", "100", "1000", "10000",
      "100000"
    )
  ) +
  ggtitle("Prior predictive distribution of minimum values")
p3 <- pp_check(fit_prior_press_ln, type = "stat", stat = "max") +
  coord_cartesian(xlim = c(0.001, 300000)) +
  scale_x_continuous("Response times [ms]",
    trans = "log",
    breaks = c(0.001, 1, 100, 1000, 10000, 100000),
    labels = c(
      "0.001", "1", "10", "1000", "10000",
      "100000"
    )
  ) +
  ggtitle("Prior predictive distribution of maximum values")
plot_grid(p1, p2, p3, nrow = 3, ncol =1)
```
- our used priors are still quite uninformative (tails of the prior predictive distributions that correspond to our normal priors are even further to the right, reaching more extreme values than for the prior predictive distributions generated by uniform priors)
- we can use summary statistics (mean,median, min, max) to test whether the priors are in a plausible range

```{r fit the model}
fit_press_ln <- brm(rt ~ 1,
  data = df_spacebar,
  family = lognormal(),
  prior = c(
    prior(normal(6, 1.5), class = Intercept),
    prior(normal(0, 1), class = sigma)
  )
)
```

```{r summary of posterior}
# parameters in log-scale
fit_press_ln
```
If we want to know how long it takes to press the space bar in milliseconds, we need to transform the Œº (or Intercept in the model) to milliseconds:
```{r}
estimate_ms <- exp(as_draws_df(fit_press_ln)$b_Intercept)

#mean and 95% credible interval of these samples
c(mean = mean(estimate_ms), quantile(estimate_ms, probs = c(.025, .975)))
```

```{r check if predicted data sets look similar to the real data set}
pp_check(fit_press_ln, ndraws = 100)
```
Now we ask: Are the posterior predicted data now more similar to the real data, compared to the case where we had a Normal likelihood? Not easy to tell.

Examining this via looking at the distribution of summary statistics:
```{r}
#not working somehow?
pp_check(fit_press, type = "stat", stat = "min")
pp_check(fit_press, type = "stat", stat = "max")
pp_check(fit_press_ln, type = "stat", stat = "min")
pp_check(fit_press_ln, type = "stat", stat = "max")
```

Minimum value in bulk of log-normal distribution and in tail of normal one; both models unable to capture the maximum value of the observed data
-> One explanation for this could be that the log-normal-ish observations in our data are being generated by the task of pressing as fast as possible, while the observations with long response times are being generated by lapses of attention. This would mean that two probability distributions are mixed here.


